# Ollama Configuration for Intel i5-13600K
# Optimized for 14 cores (6 P-cores + 8 E-cores), 20 threads

ollama:
  base_url: http://localhost:11434
  model: phi3:mini
  options:
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    num_predict: 512
    stop:
    - 'User:'
    - 'Human:'
    - 'Assistant:'
    num_ctx: 4096
    # Intel i5-13600K optimizations
    num_thread: 16  # Use 16 of 20 threads, leave some for system
    num_gpu: 0      # CPU-only configuration
    num_batch: 512  # Batch size for prompt processing
    use_mlock: true # Keep model in RAM
    use_mmap: true  # Memory-mapped files for efficiency
  timeout: 300      # Increased to 5 minutes for complex queries
  retry_attempts: 3
  
# RAG Settings with performance optimizations
rag_settings:
  chunk_size: 512
  chunk_overlap: 50
  retrieval_top_k: 5
  rerank: true
  hybrid_search: true
  embedding_batch_size: 32  # Process embeddings in batches
  max_retrieval_threads: 4  # Parallel retrieval operations

# Performance settings for Intel i5-13600K
performance:
  batch_size: 4             # Increased for better CPU utilization
  cache_responses: true
  max_concurrent_requests: 8 # Support more concurrent requests
  memory_limit: "8GB"       # Adjust based on available RAM
  
# Model-specific settings
model_settings:
  "phi3:mini":
    num_ctx: 2048     # Smaller context for mini model
    num_batch: 256
    temperature: 0.7
    
  "llama2:7b":
    num_ctx: 4096
    num_batch: 512
    temperature: 0.8
    
  "mistral:7b":
    num_ctx: 8192     # Mistral supports larger context
    num_batch: 512
    temperature: 0.7
    
  "codellama:7b":
    num_ctx: 16384    # Code models benefit from larger context
    num_batch: 512
    temperature: 0.2  # Lower temperature for code generation

# Logging configuration
logging:
  level: "info"
  format: "json"
  
# Security settings
security:
  max_request_size: "50MB"
  rate_limit:
    requests_per_minute: 60
    burst: 100